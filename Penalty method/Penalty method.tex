\documentclass{article}
\usepackage{amsmath}
\usepackage{array}
\usepackage{color}
\usepackage{graphicx}
\usepackage{float} %utiliser H pour forcer à mettre l'image où on veut
\usepackage{lscape} %utilisation du mode paysage
\usepackage{mathbbol} % permet d'avoir le vrai symbol pour les reels grace a mathbb
\usepackage{enumerate}
\usepackage{marvosym}	
\usepackage{moreverb} % permet d'utiliser verbatimtab : conservation la tabulation 


\setlength {\textwidth}{16cm}
\setlength {\textheight}{21cm} 
\setlength {\oddsidemargin}{0cm}
\setlength{\headsep}{5pt} 

\newcommand\bn{\boldsymbol{\nabla}}
\newcommand\bo{\boldsymbol{\Omega}}
\newcommand\br{\mathbf{r}}
\newcommand\la{\left\langle}
\newcommand\ra{\right\rangle}
\newcommand\bs{\boldsymbol}

\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}

\newtheorem{theorem}{Theorem}[section]

\begin{document}
\title{\textsc{\huge{Penalty method}}}
\author{Bruno Turcksin} 
\date{}
\maketitle
\tableofcontents

\section{Penalty method}
Instead of minimizing a constrained optimization problem, we solve an unconstrained problem :
\begin{equation}
\min Q_{\mu} (x) = f(x) + \frac{1}{2\mu} \|g(x)\|^2 + \frac{1}{2\mu} \|\[h(x)\]^-\|^2
\end{equation}
where :
\begin{equation}
\[h(x)\]^- = \min \{0,h(x)\}
\end{equation}
We decrease the value of the parameter $\mu$ during the optimization process.

\section{Equations}
We have the following objective function :
\begin{equation}
f = \sum_{(i,j)\in \mathcal T} \(D_{ij} - \delta_{ij}\)^2
\end{equation}
The constraints are given by :
\begin{align}
&h_1 = \gamma - \sum_{ij \in \mathcal{D}} \frac{V_{ij}}{V} \nu \(D_{ij} - \delta_{ij}\) \geq 0 \label{dv}\\
&h_2 = J \geq 0
\end{align}
Thus, we have the following quadratic relaxation :
\begin{equation}
Q_{\mu} = \sum_{(i,j)\in \mathcal T} \(D_{ij} - \delta_{ij}\)^2 + \frac{1}{2\mu} \(\[\gamma - \sum_{(i,j) \in \mathcal{D}} \frac{V_{ij}}{V} \nu \(D_{ij} - \delta_{ij}\)\]^-\)^2 + \frac{1}{2\mu} \(\[J\]^-\)^2
\end{equation}
If we want to use the steepest descent or a Newton's method to solve the previous problem, we need to take the derivative of the Heaviside function. That is a problem and thus we use the following \hbox{approximation :}
\begin{equation}
\nu (x) = \frac{1}{1+e^{-2kx}}
\end{equation}
where $k$ is a large constant.
%bjfew
The gradient of the quadratic relaxation is :
\begin{equation}
\begin{split}
\bn Q_{\mu} &= \sum_{(i,j) \in \mathcal{T}} 2 \(D_{ij} - \delta_{ij}\) \bn D_{ij} + \frac{1}{\mu} \(\[\gamma - \sum_{(i,j) \in \mathcal{D}}\frac{V_{ij}}{V} \frac{1}{1+e^{-2k\(D_{ij} - \delta_{ij}\)}}\]^-\) \times\\
&\(\sum_{(i,j)\in \mathcal{D}}\frac{V_{ij}}{V} \frac{-2k e^{-2k\(D_{ij}-\delta_{ij}\)} \bn D_{ij}}{\(1+e^{-2k(D_{ij}-\delta_ij)}\)^2}\)+ \frac{1}{\mu} \(\[J\]^-\)
\end{split}
\end{equation}
We see here that there is a problem with the penalty method. If we look at the second term of the previous equation, we see that the penalty term disappear when the constraint is satisfied (like it should) but also that the penalty term can be arbitrary close to 0 by increasing the dose. The problem is that the derivative of the logistic is very close to 0 and get closer to 0 when $x$ increases. Thus, by increasing slightly the dose, we can decrease that term faster than we decrease $\mu$.

\section{Modified constraints}
We see that we need to modified the constraints if we want to use the penalty method. To do that, we replaced the Heaviside function $\nu(x)$ by the following function :
\begin{equation}
\tilde{\nu}(x) = \left\{
\begin{aligned}
&x & \textrm{if } x\geq 0\\
&0 & \textrm{else}
\end{aligned}
\right.
\end{equation}
Thus, (\ref{dv}) becomes :
\begin{align}
\widetilde{h}_1 = \gamma - \sum_{ij\in\mathcal{D}} \frac{V_{ij}}{V} \frac{\tilde{\nu}\(D_{ij}-\delta_{ij}\)}{\delta_{ij}} \geq 0
\end{align}
where we had to renormalize the new function to keep $\gamma$ adimensional. So, we have the new quadratic relaxation :
\begin{equation}
Q_{\mu} = \sum_{(i,j)\in \mathcal{T}} (D_{ij}-\delta_{ij})^2 + \frac{1}{2\mu} \(\[\gamma - \sum_{(i,j) \in \mathcal{D}} \frac{V_{ij}}{V} \frac{\tilde{\nu} \(D_{ij} - \delta_{ij}\)}{\delta_{ij}}\]^-\)^2 + \frac{1}{2\mu} \(\[J\]^-\)^2
\end{equation}
The gradient of the constraint is given by :
\begin{equation}
\begin{split}
\bn Q_{\mu} &= \sum_{(i,j) \in \mathcal{T}} 2 \(D_{ij} - \delta_{ij}\) \bn D_{ij} + \frac{1}{\mu} \(\[\gamma-\sum_{(i,j)\in\mathcal{D}} \frac{V_{ij}}{V} \frac{\tilde{\nu}(D_{ij}-\delta_{ij})}{\delta_{ij}}\]^-\) \times\\
&\sum_{(i,j)\in \mathcal{D}} \frac{V_{ij}}{V} \frac{\[- \bn D_{ij}\]^-}{\delta_{ij}}  + \frac{1}{\mu}\(\[J\]^-\)
\end{split} 
\end{equation}
The Hessian is given by : 
\begin{equation}
\begin{split}
\bn^2 Q_{\mu} &= \sum_{(i,j)\in \mathcal{T}} 2 \(\bn D_{ij}\) \cdot \(\bn D_{ij}\)^T + \frac{1}{\mu} \(\sum_{(i,j)\in \mathcal{D}} \frac{V_{ij}}{V} \frac{\[-\bn D_{ij}\]^-}{\delta_{ij}}\) \cdot \\
&\(\sum_{(i,j)\in \mathcal{D}} \frac{V_{ij}}{V} \frac{\[- \bn D_{ij}\]^-}{\delta_{ij}} \)^T + \frac{1}{\mu} \[I\]^-
\end{split}
\end{equation}

\section{Res}

\end{document}
